Date: Aug 09, 2024

Title: The Key to Artificial Minds - Ontology

00:00:02 1
This talk in the Building Minds with Patterns series is about building mental models called 
ontologies.  

00:00:12 1
We will discuss artificial minds and mind servers. We will define intelligence and look at 
some measures of intelligence. We will define ontologies and show how they can be constructed. 
And finally, we will demonstrate how ontologies are used in reasoning. Before we dive in, please 
click the like button and subscribe to this channel if you haven't already. 

00:00:38 1
The human brain and nervous system is a complex and amazing machine, which still holds many 
mysteries.  

00:00:45 1
The human mind is likewise a mysterious and wondrous phenomenon. Current technology is only 
beginning to scratch the surface of these complex phenomena, brain and mind. Consequently, 
no claims are herein made that AGI or Artificial General Intelligence 

00:01:06 1
Instead, a modest approach is offered here for the viewer to judge.  

00:01:14 1
In building an artificial mind, we must examine the nature of mind and body, and how they relate 
to one another. Computers allow us the flexibility to change this relationship as needed. 
For example, an individual mind that controls one body could be created, but then again, so 
could a single mind that controls several bodies. Several minds controlling one body could 
provide consensus decision-making or redundancy in mission-critical situations where 
failover is required. And finally, a mind having cloned instances of itself, each controlling 
a body, may be useful in situations where the bodies need to reason similarly in order to achieve 
a collaborative goal, or where they need to share and integrate new experiences quickly. 

00:01:58 1
The focus of this talk is on crafting an individual mind for a single device situated in a real 
or virtual world.  

00:02:08 1
One could imagine minds for many kinds of devices, such as smartphones, tablet computers, 
laptop computers, and robots. A mind server is a remote control network for devices that allows 
device owners to interact with autonomous artificial minds. The mind server awaits new device 
registrations, logs in devices, and provisions new mind instances. Ideally, users could 
subscribe to a mind provisioning service using their web browser or mobile phone, and by downloading 
an app to their target device, the app would interface with a provisioned mind. The app could 
log in and retrieve a Uniform Resource Locator, or URL, of a provisioned mind instance. These 
mind instances could have access to third-party external web services, such as geospatial 
mapping or order entry services for meal delivery or airline ticketing. Or they could have 
access to external devices, such as the lights or appliances in a home. Finally, at the end of 
a session, each mind or each user profile could be persisted so that when the session is concluded 
and a user returns, the user interaction could be resumed. 

00:03:25 1
The first question we must ask is, what is intelligence? Everyone has asked this question and 
everyone has answered it. 

00:03:33 1
Jean Piaget wrote, the mechanism is the intelligence. We will unpack what that means shortly. 


00:03:45 1
We can make a provisional definition as follows. Intelligence is the ability to acquire and 
apply knowledge and skills. 

00:03:53 1
Knowledge is facts and information.  

00:03:57 1
Skills are reliable groups of behaviors and actions.  

00:04:02 1
Measuring the development of a human mind has its challenges. Because infants cannot readily 
report on their perceptions, it's long been assumed that their perceptual abilities were 
poor, just like their motor skills. Robert Fantz in 1961 invented the preference method, which 
measures how long an infant looks at either of two simultaneously presented figures. 

00:04:33 1
Additional means of evaluating infant perception were developed since then, including heart 
rate monitoring and sucking action monitoring. The ability to assess the cognitive development 
of an artificial mind remains a challenge. At this time, there are no consensus standards for 
assessing embodied artificial minds. Psychologists, however, have developed many kinds 
of assessments for humans. 

00:04:56 1
Among them are the Bailey Scales of Infant and Toddler Development, the Developmental Milestones 
invented by Jean Piaget, and the Cattell-Horne-Carroll Framework for Intelligence Assessment. 
Let's look at each one. 

00:05:12 1
The Bailey Scales of Infant and Toddler Development is an assessment instrument used to gauge 
human developmental functioning.  

00:05:23 1
The third edition, Bailey 3, covers cognitive, language, socio-emotional, motor, and adaptive 
behavior areas of human development.  

00:05:34 1
Through his well-documented observations, Jean Piaget has identified numerous milestones 
attained by his subjects at specific ages, which provide markers for general human development. 
 

00:05:50 1
Note that number colon number indicates years and months. For example, 2 colon 3 means 2 years 
and 3 months of age, while 0 colon 0 means birth. 

00:06:05 1
The Cattell-Horn-Carroll, or CHC, model of intelligence is a framework that summarizes the 
components of human general intelligence.  

00:06:19 1
The CHC model has undergone several revisions, but remains the most widely accepted contemporary 
theory of cognitive abilities.  

00:06:27 1
Andreas Dimitriou, from Cyprus, and others have refined and expanded this model from a neo-Piagetian 
perspective.  

00:06:37 1
Fluid reasoning is one ability that is particularly important, as well as comprehension knowledge. 
 

00:06:49 1
We can say that for an artificial mind, memory represents the mind's crystallized intelligence, 
while mechanisms represent the mind's fluid intelligence. For an artificial mind, also called 
a cognitive system, we can say that it is comprised of a memory, which contains the world model, 
possibly in the form of an ontology, as well as adaptive mechanisms, which Piaget alluded to 
in his earlier quote. 

00:07:22 1
What is an ontology? Buddha said, with our thoughts we make the world. 

00:07:30 1
Let's explore the definitions. In metaphysics, ontology is the philosophical study of being 
that investigates what type of entities exist, how they are grouped into categories, and how 
they are related to one another on the most fundamental level. 

00:07:48 1
Tying it back to artificial minds, we can say that an ontology is a world model containing knowledge 
and skills which, combined with adaptive mechanisms, constitute a cognitive systems intelligence. 
 

00:08:10 1
As human beings, memory is the most important thing we have. Memory connects our past with our 
future, and enables coordination and reflection. 

00:08:21 1
Each person is a living library of experience, which unfortunately burns to ashes when she 
or he dies. Our memories are personal. No one knows the experience another person has at any 
given moment. Memory is shareable through speech or physical performance, and through technologies 
such as writing, art, audio or video recording, and so on. But communication can only share 
shards of the experience. It cannot convey the full affect or weight of the original experience 
upon the experiencer. And, except for those people with eidetic memories, as time progresses 
the original experience fades, leaving a person with only glimpses of his or her own personal 
history. 

00:09:05 1
What is memory? A functional answer is that memory is the place to which information is stored 
and from which information is retrieved. The biological and mechanical answers to this question 
are very different. 

00:09:20 1
Let's take a look at biological memory.  

00:09:24 1
A neuron or nerve cell is a biological unit that sends and receives electrical and chemical 
signals. The signals cross gaps between neurons called synapses. Neuroglia, or glial cells, 
keep neurons in place, insulating them from each other while providing oxygen, nutrients, 
and sanitation. Memories are groups of neurons, which fire together in the same pattern each 
time they're activated. The links between individual neurons, which bind them into a single 
memory, are formed through a process called long-term potentiation. 

00:09:56 1
When an individual neuron fires, it temporarily enables neighboring neurons to fire as well. 
This temporary period may last from several hours to several days. A refiring of the same neuron 
will trigger the enabled neighbors to fire in synchrony. This in turn may recruit even more 
neighboring enabled neurons to fire in the future. This is the essence of long-term potentiation. 


00:10:24 1
The electrical signals cross gaps between neurons called synapses.  

00:10:37 1
Several brain regions relating to human memory are outlined in this diagram. The main areas 
involved in memory are the cerebellum, putamen, caudate nucleus, hippocampus, amygdala, 
cortex, frontal lobe, and temporal lobe. 

00:10:58 1
The putamen and caudate nucleus are components of the dorsal striatum.  

00:11:05 1
The caudate is also referred to as the medial dorsal striatum, while the putamen is also referred 
to as the lateral dorsal striatum. The dorsal striatum, which is within the basal ganglia, 
is involved in motor action planning, intrinsic reward, motivation, and reinforcement. 
The basal ganglia itself is generally thought to be a brain region involved in action selection, 
learning, thinking, and emotion. 

00:11:34 1
Habits, actions, episodes, fears, and facts are all said to be different kinds of memories 
that are stored in the brain. The caudate nucleus is thought to be responsible for storing habits. 
Deficits in the caudate nucleus show that it is also involved in motoric inhibition. 

00:11:54 1
The pudiment and cerebellum store actions or procedural memory. The hippocampus stores episodes 
which are then moved to the cortex where they are ultimately retrieved by the frontal cortex. 


00:12:08 1
The amygdala projects to the caudate nucleus as well as the hippocampus. Fears are stored in 
the amygdala. 

00:12:18 1
Facts are stored in the cortex and then are transferred to the courtesies of the temporal lobe, 
and are retrieved by the frontal lobes.  

00:12:32 1
Let's contrast biological memory with mechanical memory.  

00:12:37 1
Mechanical memory is quite different indeed. Computers have two basic kinds of memory, volatile 
memory, which is erased when the power is turned off, and nonvolatile memory, which retains 
its informational configuration even when power is turned off. 

00:12:54 1
Random access memory, found on silicon-based integrated circuit boards, is volatile.  

00:13:04 1
Secondary storage is typically non-volatile, and found in hard-spinning disk drives or solid-state 
disk drives.  

00:13:12 1
Virtual memory is memory that is managed by a computer's operating system, system, typically 
spanning both volatile and non-volatile memory.  

00:13:22 1
Computer scientists have developed many data structures to store and retrieve information. 
Some of these information storage data structures usually have elements called units or nodes 
that combine to form networks or graphs. A plethora of such memory networks have been created 
to store and retrieve information, each with different characteristics. Generally speaking, 
these network structures can be categorized as artificial neural networks and alternative 
memory networks. 

00:13:49 1
The neural networks can be further subdivided into weighted and weightless neural networks. 
 

00:13:59 1
Weighted neural networks rely upon threshold logic units, a computational unit where numerical 
inputs to the unit are summed and multiplied by a weighting factor. This is in order to determine 
whether or not the unit should fire. If the unit's result exceeds a certain threshold, then 
the unit can fire, or return a positive value. Otherwise, it does not fire, and it simply returns 
a zero value. In threshold logic units, synaptic connections attach directly to the somas 
of weighted neurons. 

00:14:31 1
This approach, espoused by McClelland and Pitts, is the dominant approach in the current neural 
network paradigm. Typically, these approaches involve a two-step process to use the network. 
An initial training phase, which presents example features to the network in order to adjust 
the weights among unit connections, after which the network is frozen. Then a second real execution 
phase devoid of learning is performed, wherein input features are presented to the network 
in hopes that the network provides the correct results. More recent approaches combine these 
two phases to provide continual online training and execution. 

00:15:15 1
Please allow us to digress for a few moments. You may get a cup of coffee or skip to the next slide. 


00:15:22 1
The input features are often represented as an array of numeric values. A gradient is a derivative 
applied to a vector, an array of values. A gradient determines the slope of a number of values 
at once. There are several approaches to bias modification in weighted neural networks. 

00:15:46 1
This type of learning works well when the inputs to the network are mutually independent. Delta 
modification, also called least-mean-square modification, alters weights by comparing 
input weights to desired output weights and reducing any difference between the two in the 
activated units of the network. Delta learning works when no hidden units exist in the network. 
When there are hidden units, then the errors need to be propagated backward from the output 
units to the input units across the hidden units. For delta modification to work, the desired 
output weights must be known in advance. Weights are propagated in a fashion similar to that 
in the Large-Scale Memory Storage and Retrieval, or LAMP star, neural network. 

00:16:28 1
Okay, now we can resume.  

00:16:31 1
Typically, weights are adjusted in a training phase, and then the network is frozen and run 
to perform classification or control tasks. Rather than having two separate phases, an online 
approach is needed where nodes are continually updated. 

00:16:49 1
Weightless neural networks rely upon synaptic connections between artificial neurons and 
artificial dendrites, often employing random-access memory nodes.  

00:17:01 1
Alternative memory network approaches involve memory units that are not called neurons. 
Instead, they may be decision trees, bit vectors, biomimetic neurons, or a range of other data 
structures. 

00:17:15 1
Given what we've discussed so far about biological and mechanical memory, let's try to formulate 
some requirements.  

00:17:23 1
Some questions to be asked are, how should the memory of an artificial mind be organized? What 
are the elements comprising such a memory? What is the layout? Since there are many memory models 
from which to choose, some initial considerations for the uses of the memory must be gathered 
and made explicit. Furthermore, any problems arising from an inadequate specification should 
also be brought to light. 

00:17:46 1
For an artificial mind, the first job of memory is to support perception.  

00:17:51 1
Somatosensory percepts such as video, audio, joint positions, and other sensations emanating 
from a device should be stored with high fidelity in the memory. Preconceptual thought is grounded 
in sensory motor experience, with little or no abstraction. Therefore, concrete representations 
should be facilitated. As the age of the system progresses, the ability to abstract information 
should improve, and therefore abstract symbolic concepts should also be allowed to be stored. 
Information compression strategies are needed to go from the high-fidelity percepts to abstract 
situations and concepts. 

00:18:27 1
The memory should persist non-deleted information for the lifespan of the cognitive agent. 
 

00:18:33 1
The memory should support situational contexts. This means that it should be possible to construct 
an abstract environmental description of the present situation, and that the currently active 
description should not be confused with past descriptions. Therefore, a way to filter out 
descriptions irrelevant to the current context must be devised. 

00:18:55 1
To that end, activation of the contextually relevant portions of memory should be feasible. 
 

00:19:01 1
The memory should support the formation of simple and complex relationships. Simple, often 
called horizontal relationships, are usually between two entities. Complex, often called 
vertical relationships, are usually among more than two entities. 

00:19:19 1
Viewing a situation from another's point of view is called perspective-taking. It involves 
the ability to either imagine the sensory experience of others, or the ability to ascribe beliefs, 
emotions, and intent to others. This ascriptive ability is also called theory of mind. The 
memory should facilitate perspective-taking and theory of mind. When individuals have contradictory 
opinions about a subject, for example, when Mary thinks cake is delicious and John thinks cake 
is not delicious, these contradictory positions should also be supported in the memory. 

00:19:52 1
Dialectic, logical, probabilistic, and other alternative reasoning methods should be easily 
supported by the memory.  

00:20:00 1
Furthermore, problem solving with possible worlds representations should also be capable. 
The memory should be concurrent, allowing multiple processes to act upon different parts 
of the memory. 

00:20:21 1
Analogy via structure mapping, metaphor, and other kinds of symbolism should be supported. 
 

00:20:28 1
The system should be able to decide if new propositions are coherent with existing propositions 
and simultaneously hold multiple points of view.  

00:20:43 1
Consolidation, compression, automaticity, and forgetting should be supported.  

00:20:48 1
The binding together of experiences within disparate parts of the memory, long-term potentiation, 
should also be facilitated.  

00:20:59 1
The memory is the mind's crystallized intelligence while the mechanisms are the mind's fluid 
intelligence.  

00:21:07 1
To address the aforementioned memory requirements, the knowledge representation chosen 
in this approach is a neurosymbolic representation called neural propositions.  

00:21:19 1
A colligative structure is one that is formed by binding together other elemental structures 
in much the same way that a molecule is formed by atoms binding together.  

00:21:29 1
A neural proposition and memory unit is a colligative structure having both neural and symbolic 
aspects. This hybrid approach facilitates both symbolic programming to select, connect, 
and manipulate the memory units at varying levels of abstraction, while also facilitating 
activation among the memory units. 

00:21:50 1
The synaptic colligatives, or linking elements, in this case are called monads, arguments, 
which serve as identifiers or reifiers that affirm or negate each proposition. The container 
portion is called a scheme and is a structure that binds together a semantic relationship with 
optional afferent arguments that can provide a domain for the relationship and optional efferent 
reference that can provide a range for the relationship. 

00:22:19 1
A collection of schemes forms a subsystem, also known as a schema.  

00:22:27 1
We'll explore the first memory unit, the monad.  

00:22:34 1
According to Leibniz, a monad is the basic substance or building block of reality.  

00:22:51 1
Jean Piaget was very interested in Leibniz's theory of monads. In this conceptualization, 
there are two kinds of monads, affirmations and negations. 

00:23:02 1
In Piaget's theory of child development, affirmations and negations represent observed 
and inferred characteristics of objects. According to Piaget, affirmations are the first 
cognitive structures. An affirmation represents the unique existence of the cognitive structure. 
In the physical realm, the objects that an individual perceives display only positive characteristics 
such as surfaces, weight, feel, etc. Only those positive attributes of the object can be observed 
and incorporated into schemes. These positive characteristics are primary. 

00:23:40 1
A negation represents a unique non-existence of the structure. One or both may be associated 
with a cognitive structure at any given time. Affirmations are typically constructed first, 
while negations are often constructed later. 

00:23:58 1
The invisible negative characteristics arise from the object only as a function of expectations 
or predictions made about the object.  

00:24:15 1
As a consequence, early in development there is an asymmetry between the affirmations, that 
is, the primary positive characteristics, and the negations, that is, the secondary negative 
characteristics. As negative characteristics are discovered, actions are more likely to 
be reversed successfully. Negations structure exclusions and aid action refinement. Collectively, 
we can call both affirmations and negations reifiers. Reifiers provide a way to discuss the 
affirmative and negative aspects of anything. 

00:24:48 1
Reifiers can be represented by numbers. We can utilize a 128-bit signed integer to represent 
the affirmations and negations. Each affirmation can be a positive integer, and the corresponding 
negation can be a negative integer. So, to create an affirmation, a counter indicating the 
maximum number of available reifiers is required. Starting from zero, the counter is incremented. 
A positive number can reference the affirmation, while a negative number can reference the 
corresponding negation. Using this strategy, the aspects of things in the world can be represented. 


00:25:22 1
Incrementing the reifier count can create more affirmations and negations to represent at 
various aspects of the world.  

00:25:30 1
Since the number of elements in the world could get big, the ray of fires can be displayed in other 
than base 10, for example, in base 36.  

00:25:40 1
In base 36, the letter A represents the decimal number 10, while the letter Z represents the 
decimal number 35.  

00:25:49 1
128-bit integer can be fully represented using approximately 21 digits in base 36.  

00:25:58 1
As stated earlier, a synapse is a transmission site for electrical impulses between two nerve 
cells.  

00:26:05 1
By enumerating artificial synapses, they can be treated like reifiers.  

00:26:22 1
A pair of opposing reifiers is called a dyad.  

00:26:28 1
In a dyad, the affirmative monad is called the thesis, while the negative monad is called the 
antithesis.  

00:26:36 1
For example, if the affirmation, or thesis monad, in a dyad represents A, the negation monad, 
or antithesis, will represent non-A. Likewise, in the artificial mind, monads play the role 
of affirmations and negations. Thesis monads serve as affirmations, while antithesis monads 
serve as negations. 

00:26:58 1
In a typical Euler or Venn diagram, a universe is represented as either a square or a circle. 
If a set A belongs to a universe, anything that is non-A is usually depicted as being outside 
the set. 

00:27:12 1
If we are to be consistent, then the set A would be our thesis, or the affirmation, and the non-A, 
outside A, would be our antithesis, or the negation. We are about to get a little wonky here, 
so feel free to advance to the next section and bypass this discussion. 

00:27:32 1
When a set A is comprised of A1 and A2, the negations are implicit.  

00:27:40 1
A1 is all of A but excluding A2.  

00:27:45 1
A2 is all of A but excluding A1.  

00:27:50 1
This is not easy to see in a typical Euler diagram.  

00:27:57 1
But what if non was an entity rather than a function?  

00:28:04 1
Now we see that A consists of all of A, A1, non-A1, A2, and non-A2 all at once.  

00:28:14 1
Typically non-A1 and non-A2 are functions. They are operations on A1 and A2 respectively. 


00:28:21 1
What if non-A1 and non-A2 were explicitly made into entities instead of functions?  

00:28:28 1
This would be done to make things more explicit and easier overall.  

00:28:35 1
In Piaget's theory of child development, affirmations and negations represent observed 
and inferred characteristics of objects.  

00:28:44 1
Piaget said that it's easy for people to acquire the positive characteristics of things, but 
negative aspects are laboriously constructed. So to make things simple, A1 and non-A1, and 
A2 and non-A2 are defined explicitly. 

00:28:57 1
Furthermore, the affirmative aspect of such sets will provisionally be called a thesis, while 
the negation will be called the antithesis.  

00:29:08 1
So that would make A1 a thesis, and it would make non-A1 its antithesis.  

00:29:16 1
So for any sets A, B, C in a universe the A, B, C are theses. The non-A, non-B, and non-C are antitheses. 


00:29:33 1
And so on ad infinitum ad This brings us back to the notion of dyads we discussed earlier.  

00:29:48 1
Let's talk more about this colligative structure. We've explored the synapses of this colligative 
structure with monads and reifiers. A scheme is a reified pattern. It is a container structure 
that has its own affirmation and negations, and in addition, it can reference other affirmations 
and negations. 

00:30:07 1
Each scheme has a reifier which uniquely identifies the scheme.  

00:30:12 1
Each scheme has a relation which ties together the other components of the scheme.  

00:30:19 1
And each scheme has terms. These are arguments and reference. Each key has zero or more arguments 
which serve as inputs to the scheme. Each scheme has zero or more reference which serve as outputs 
of the scheme. 

00:30:37 1
Some schemes may form a pure hierarchy and therefore have no reference, while some schemes 
may be atomic and have neither arguments nor reference.  

00:30:48 1
Schemes can be viewed in a neural or propositional manner. Viewed as a neuron, we have the axon, 
which is bifurcated or split into two. We have the soma, which represents the body or the relationship, 
and we have dendrites, which are both afferent and efferent, meaning they take inputs or they 
produce outputs. 

00:31:10 1
Viewed as a proposition, the reifiers create an ID or a handle for the proposition. The proposition 
has a relation, and the proposition has terms, which can be arguments or reference, inputs 
or outputs. 

00:31:30 1
Recall that a biological neuron has these parts as well.  

00:31:36 1
When we take a look at the propositional aspect, we can represent a proposition with relation 
P and terms X, Y. Here, X is an argument and Y is a referent. 

00:31:49 1
We can represent unary propositions where we just have R of X and there's R where X is the argument 
and there is no referent.  

00:32:02 1
Schemes can represent binary propositions, where R of X equals Y, or simply R of XY, with two 
arguments and no referent.  

00:32:14 1
We can also represent ternary propositions, where R XY equals Z, where X and Y are arguments 
and Z is a referent.  

00:32:31 1
The terms X, Y, and Z may be either arguments, reference, or a combination of both. Nullatic 
propositions with no arguments or reference serve as atomic propositions. So, a scheme is 
a basic structure that can support propositions, and is analogous to a neuron, hence the term 
neural proposition. The internal layout of a scheme is a bit more complicated, and we won't 
quite get into that in this video. Suffice it to say that it is designed to support the aforementioned 
requirements. 

00:33:06 1
Within a cognitive system, we have devices that are attached. When mapping a scheme to a device, 
we can consider three kinds of uses, representing representing entities external to the device, 
representing entities internal to the device, and representing inferred entities. 

00:33:25 1
An external scheme represents an exterocept, which is a perception from outside the body of 
the individual. An internal scheme is a perception from within the body of the individual. 
There are several kinds of internal schemes. Kinesthetic schemes, also called propriocepts, 
indicate what an individual is doing, while emotional, synesthetic, and postural schemes, 
collectively called interoceps, indicate what an individual is feeling. External and internal 
schemes can be referred to as observables. 

00:33:54 1
Observables are immediate sensory perceptions believed by an individual.  

00:34:02 1
According to Piaget, there are two important kinds of observables. Proprioceps, which are 
internal sensations of bodily movements being performed, and exteroceps, which are external 
sensations of perceived objects. 

00:34:16 1
External and internal schemes can be referred to as observables, while inferential schemes 
are often referred to as coordinations. An inference, that is inferential scheme, represents 
a collection of one or more other schemes. An inference may be referred to by either its affirmation 
or negation. 

00:34:38 1
Agent components, called mechanisms, utilize the observables and coordinations to build 
the ontology.  

00:34:45 1
Assimilation mechanisms form new schemes.  

00:34:50 1
Accommodation mechanisms revise existing schemes.  

00:34:54 1
A subset of coordinations within the cognitive system are called disturbances. Obstacles, 
gaps, novelties, objectives, and contradictions are special category of ephemeral coordinations, 
which drive change in the cognitive system because they are disturbing. These are not addressed 
further in this presentation. 

00:35:14 1
Collections of schemes are called schemas or schemata.  

00:35:22 1
A subsystem, otherwise called a schema, is a collection of schemes. Two subsystems may be collateral, 
meaning on the same horizontal level, or hierarchical, meaning vertically linked. 

00:35:40 1
Together, all subsystems form a totality.  

00:35:47 1
Schemas may be used to represent a partial ontology. Consider the following facts. 

00:35:56 1
Albert Einstein was a physicist. Biologists and physicists are scientists. Scientists are 
academics. 

00:36:08 1
This subsystem of facts can be represented as follows.  

00:36:16 1
We can create a glossary or lexicon for each of the simple or compound words. Each of the simple 
or compound words are called lexemes. 

00:36:28 1
For each lexeme, we can internally substitute a reifier to represent that lexeme.  

00:36:36 1
For the propositional schemes, we can define their input arguments and output reference, 
and represent those propositions by the corresponding reifiers.  

00:36:48 1
Finally, we can associate the arguments of each atomic scheme with its corresponding proposition. 
 

00:36:59 1
So the facts can be represented as neural propositions, with both a human-readable representation 
or a machine-readable representation.  

00:37:14 1
Let's now look at relations.  

00:37:19 1
Some relationships, for example, PX equals Y, can be represented by an individual scheme, 
while other relationships, for example, PXYZ, can be represented by a schema, that is, a subsystem. 
 

00:37:31 1
Let's take behaviors as an example.  

00:37:35 1
What are behaviors? For discussion purposes, the term action, behavior, or rule are interchangeable. 
The term behavior will be used to hopefully minimize confusion. A behavior is a structure that 
performs one or more actions. The action a behavior performs may be primitive or atomic or composite 
and complex. Behaviors typically marshal an ensemble of fine-motor actions. For a device, 
these fine-motor actions are called actuations. An action can occur in one or more modalitiesâ€”visual, 
phonal, tactile, kinesthetic, and so on. Actions are causal and often exert forces upon objects. 
Actions Actions may also be recognitive, that is, used to recognize objects. 

00:38:19 1
We can imagine a chair recognizer action that recognizes all kinds of chairs. When faced with 
something that is chair-like, it will decide yes, that's a chair, or no, that's not a chair. 


00:38:30 1
A behavior is also called an action schema.  

00:38:35 1
A behavior typically has three parts. A context, which indicates preconditions for triggering 
the behavior, one or more primitive or composite actions to be performed, and a result which 
indicates the predicted or expected post-conditions upon successful performance. The context 
specifies conditions that exist in the current situation. Objectives are conditions that 
are desired but may not yet exist in the current situation. Typically, objectives appear in 
the result portion of a behavior. The actions to be taken may be primitive, that is, atomic or 
indivisible, or composite, in other words, they contain other actions. 

00:39:13 1
In a constructivist cognitive system, a potentially infinite number of condition elements 
are dynamically synthesized. They can be represented as numbers, usually integers, or literals, 
alphanumeric constants. The condition elements can also be negated to demonstrate the non-occurrence 
of a particular element. 

00:39:31 1
In our conceptualization, a behavior has a specific ray of fire which identifies that specific 
behavior. In non-constructivist cognitive systems, a finite number of condition elements, 
usually called tokens, are defined a priori, so that the behavior context and results are often 
simply lists or vectors of numeric tokens. 

00:39:50 1
In artificial intelligence parlance, there are two types of behaviors, policies and macro 
operators. For the most part, policies and macro operators are largely equivalent. However, 
these terms are favored by different artificial intelligence communities. The reinforcement 
learning community prefers the term policy, while the symbolic learning community prefers 
the term macro operator. Given a context containing preconditions P, not Q, R, a primitive 
action A, and a result containing postconditions X, Y, such a behavior can be represented as 
either a policy or a macro operator. Policies typically specify only the context and a single 
primitive action to be performed, while macro operators specify the context, one or more primitive 
or composite actions, and the result. 

00:40:36 1
A policy can be represented by a collection of schemes, or a schema.  

00:40:44 1
For example, atomic schemes can represent the context elements and action to be taken, while 
propositional schemes can represent the slots or attributes of the behavior.  

00:40:55 1
So a policy schema can be internally represented within a cognitive system, as a schema or a 
subsystem of schemes.  

00:41:06 1
Having a translation table for the simple and composite words or condition elements or relations 
involved, we can in turn generate a completely internal machine readable version of the policy. 
Such an internal version can be represented by a lexeme table, a scheme table to represent the 
propositions, an argument table to represent inputs to propositions, and a referent table 
to represent outputs of propositions. These tables are sufficient and can stand on their own 
to represent the policy. 

00:41:41 1
In a similar fashion, a macro operator can also be represented by a collection of schemes. Given 
a macro operator transformed into a human readable schematization, the macro operator itself, 
in a concise visual representation, can be expanded to a readable schema representation and 
then lexified to extract the simple word, context conditions, and attributes. 

00:42:04 1
Using the lexicon, a machine-readable version can be produced, and the propositional schemes 
identified, and the arguments identified, and the reference identified, so that the tables 
are equivalent to the original behavior. So, both policies and macro operators can be represented 
as schemas. 

00:42:30 1
To recap, a behavior, also called an action schema, is a subsystem that works together to perform 
a specific complex action. Action schemas typically marshal an ensemble of primitive fine 
motor behaviors, or actuations, to execute a more complex behavior. An individual action 
can occur in one or more modalities, for example, visual, phonal, tactile, kinesthetic, and 
so on. Action schemas are physical and exert forces upon objects. Actions can be said to have 
gaps because all possible ways the action can be performed are not initially known. These possibilities 
need to be filled in over time with experience. An action group is a system of actions or operations 
and their inverses. Jean Piaget defines a group as a set of operations such that they can be composed. 
Any two will produce a third belonging to the same set, and the set contains each operation's 
inverse. A group may be practical, where the perceived objects associated with the actions 
have no relationship to each other. Subjective, where the objects associated with the actions 
are experienced relative to the performer of the action. Objective, where the objects associated 
with the actions are experienced relative to other objects involved in the actions and not 
relative to the performer. Or representative, where objects associated with the actions 
are internalized, mental representations as opposed to physical objects. 

00:43:51 1
A skill is a collection of actions, or operations, that may include action groups. A skill is 
also known as a schema of assimilation. 

00:44:01 1
Some skills are endowed while others are acquired. Cognitive processes often exercise skills 
when they are new, or when a skill causes interesting phenomena. There are genetically endowed 
or instinctual skills, and experientially acquired or habitual skills. 

00:44:21 1
Let's now turn to the totality.  

00:44:25 1
The totality is the total collection of schemes an individual has, comprising all the skills, 
operations, actions, and other subsystems. The totality can also be called the ontology, 
the world model, or in this case, the neural matrix. 

00:44:45 1
The word matrix means womb, a place where something is created, in this case a mental model. 
The mental model is implemented as a knowledge base, that is a database, following Roland Hauser, 
and is visualized as the neural matrix, a vast tiered heterarchy with multiple apices, or tops. 
The mental model contains regions for perception, features, entities, classes, situations, 
stories, episodes, and actions. Percepts, results, and urges function as observables in 
the mental model. Specifically, percept and feature propositions play the role of both exteroceptive 
and interoceptive observations. Result messages and event propositions play the role of 
propriocepts. Urge propositions play the role of homeostatic interocepts. All other propositions 
function as either coordinations or disturbances, the difference being that disturbances 
are the most ephemeral, while coordinations are expected to persist within the cognitive 
system. 

00:45:44 1
A totality has several dimensions.  

00:45:48 1
Schemes can be envisioned as residing within a sparse multidimensional dynamic Euclidean 
matrix. The matrix itself is dynamic, since it begins with a single scheme and expands or contracts 
along each axis as neurons are added or removed. The matrix is organized into tiers along three 
axes, the abscissa, X, the ordinate, Y, and the applicative, Z. The boundaries of the matrix 
form a box. Each Z axis unit is called a tier. 

00:46:18 1
Each X axis unit is called a column. Each Y axis unit is called a row. 

00:46:25 1
And in addition to these axes, there are several additional dimensions, realities and viewpoints, 
which are not captured spatially. Realities are dimensions along which activation can travel. 
Realities partition the matrix with respect to activation. Examples of realities are the 
observed reality, which is empirical sensory experience. We call that the current model. 
And desired reality, the desired future reality. 

00:46:51 1
There's also the possible reality, which is used for hypothetical reasoning and self-directed 
problem solving. And there are other realities as well. 

00:47:01 1
Viewpoints partition the matrix in terms of coherence and consistency. Viewpoints are reasoning 
contexts, similar to Doug Linnet's micro-theories. 

00:47:12 1
Viewpoints may be locally coherent and consistent, despite being globally incoherent or 
inconsistent.  

00:47:18 1
Note that the overall network is not fixed. Rather, it is continually reconfigured based on 
the mechanisms operating upon it. 

00:47:27 1
The elements of the totality are activated using time. Each memory element is timestamped 
to indicate the element is active. As time advances, the elements decay until the time exceeds 
a certain threshold called the activation window. 

00:47:42 1
When the elements are within the activation window, the elements are considered active. When 
the elements are outside the activation window, the elements are no considered active. Multiple 
activation windows are used for different purposes. For example, a forgetting window is used 
to determine whether or not to consider an element forgotten, while a perception window is 
used to determine whether or not to consider a percept as relevant. 

00:48:12 1
We can now look at various regions of the ontology.  

00:48:19 1
The ontology is a morass of heterarchical relationships. It's envisioned that billions of 
schemes will reside in the totality. The schemes should tend to cluster in various regions 
based on the predefined relationships that will bootstrap the overall cognitive system. 
New relationships will be necessarily created from the predefined relationships. 

00:48:39 1
If we collapse the totality into two dimensions, we can begin to identify these regions.  

00:48:51 1
These relationships are involved in mental simulation, discovery, learning, story understanding, 
imitation, play, and so forth. Mike Dyer wrote about some of these in his work. Some of the predefined 
relationships are depicted above. 

00:49:08 1
The behavioral region deals with motoric actions and the consequences of actions and problem 
solving. Some of the relationships are defined here. 

00:49:21 1
The next highest level is conceptual.  

00:49:25 1
This level deals with concept formation and learning over time.  

00:49:30 1
Concepts are different from categories or entities in that the properties of concepts may 
be unknown.  

00:49:36 1
The episodic region is the next level down and deals with unifying experiences. The relationships 
here deal primarily with long-term potentiation. 

00:49:47 1
The event region deals primarily with capturing momentary experience and situation formation. 
 

00:49:54 1
Homomorphisms among events, analogy, and metaphor are available in this region.  

00:50:00 1
Some predefined relationships in this region are shown.  

00:50:06 1
The object region is for coalescing multimodal features into entities or objects. Multimodal 
features of perceptions are coalesced into entities or objects. Some relationships are depicted. 


00:50:21 1
The feature region deals primarily with extracting multimodal features from percepts, a 
percept being a video clip or an audio clip or other observable from the device. Features such 
as phonemes, edges, colors, shapes are disassociated in this region. New features are synthesized. 
Some relations in this region are as follows. 

00:50:49 1
We've said before the memory is the mind's crystallized intelligence while the mechanisms 
are the mind's fluid intelligence.  

00:50:59 1
Jean Piaget outlined two fundamental adaptive processes for individual development, to 
which a third will be added.  

00:51:07 1
Assimilation is the creation of schemas using existing reifiers.  

00:51:12 1
Accommodation is the revision of schemas, leading to new schemas, through a process of copying 
and substitution. An assimilation mechanism creates a new scheme or schema based on existing 
reifiers. For the external observed atomic scheme B'' and the proprioceptive internal atomic 
schemes B2 and C, a new inferential scheme unites these existing schemes as arguments. The 
new scheme represents some relationship among the observables. 

00:51:44 1
Basic assimilation can be based on synchrony, observables occurring at the same time, otherwise 
called spatial pooling, or diachrony, observables occurring in series, one after the other, 
often called temporal pooling.  

00:52:01 1
Accommodation involves modifying existing schemes or schemas. Accommodating an existing 
schema is a matter of cloning it and modifying the clone, thereby preserving the original. 
This can be repeated as many times as needed. For example, schemes within an existing subsystem 
used in grasping small objects may be cloned and modified to grasp large objects. Over time, 
other schemes in the subsystem may also be cloned and modified to grasp medium-sized objects. 
In this way, newly discovered internal or external elements can be incorporated without disrupting 
existing structures. Two basic adaptive processes are necessary. Assimilation, which is 
schema creation, forms new schemas using existing reifiers, and accommodation, schema revision, 
forms new schemas by editing copies of existing schemas. 

00:52:51 1
A third process can be added, consolidation, or garbage collection, which is the disposal 
of existing schemas.  

00:53:02 1
The subsequent mechanisms will employ assimilation accommodation, or consolidation.  

00:53:11 1
Observation mechanisms add observables to the totality.  

00:53:17 1
Will Rogers famously said, people's minds are changed through observation and not through 
argument. The same goes for machines. Observation is the process of receiving and storing 
somatosensory stimuli in memory. The stimuli are called observables. In the observation 
pattern, observables are cached or persisted so that other modules may act upon them. 

00:53:41 1
The perceiver module stores the observables in memory. And this pattern is useful when there's 
a need to share observables with multiple modules or when a historical perceptual trace is 
required. 

00:53:53 1
A psyche mechanism located on the device as an app brokers observables originating from the 
device and commands originating from the mind. The perceiver receives observables from the 
psyche. The executor sends actuation compounds to the psyche. The storer catalogs and indexes 
the observables, whether video, audio, and so forth, and facilitates retrieval. 

00:54:22 1
The retriever fetches schemas from memory, whether short-term, long-term, or associative 
memory. In the current environment, a web crawler can function as a storer, and a large language 
model can function as a retriever. 

00:54:38 1
Mechanisms for coordination add inferences to the totality. Coordination is the process 
of adding inferences to memory. Inferences can be logical conclusions, plans, behaviors, 
and so forth. The activator agents are responsible for transferring activation among propositions. 
In biological systems, certain cells called excitatory cells have the capacity to generate 
and transfer electrical impulses amongst themselves. This is known as spiking or firing. 
In artificial neural networks, this phenomenon is mimicked by the assignment of weights to 
nodes in the network. In semantic networks, a process called spreading activation makes certain 
nodes in the network relevant when contextual cue concepts are presented to the network. This 
imitates the priming effect found in cognitive psychology, that is, when people recall information 
quickly once a related cueing concept is presented to them. In all cases, each cell or node is 
presented with a stimulus and other cells or nodes are affected in certain biological or artificial 
ways. Activation is said to transfer from one cell or node to another. The notion of activation 
in here is quite similar. For propositions contained into totality, activation time is transferred 
from one proposition or viewpoint to the next based on several factors. Afferent and efferent 
terms are defined by the pre-configured predicate and term instances for the relation prototype. 


00:56:00 1
An observable proposition is one that is created by the perceiver agent. Observable propositions 
do not have efferent terms, only afferent terms. So events, percepts, and so forth will simply 
store activation. A coordination proposition is one that is asserted by a coordination agent. 
Most coordination propositions have both afferent and efferent terms. The activator agents 
are responsible for transferring activation among propositions or viewpoints. 

00:56:27 1
Graphical models called Bayesian networks are used to compute the probability of an outcome 
given an explicit causal model of the outcome's quantitative and qualitative dependent variables 
construed in terms of conditional probabilities. The entire Bayesian network defines a joint 
probability function over the nodes in the network. Each node in the network contains a probability 
value referred to as a belief. Nodes with known values are observable and nodes with unknown 
values are hidden. The belief propagation algorithm invented by Uta Perl allows the approximate 
computation of beliefs in linear time to the number of nodes in the network. Each proposition 
in the totality has associated statistical belief variables. The purpose of these variables 
is to localize information such as desirability, likelihood, reliability, and so forth about 
a particular proposition so that the agents utilizing the proposition can leverage quantitative 
statistics about the proposition to make informed decisions. Belief propagation, therefore, 
is the means by which a statistical information about each proposition is updated within the 
system. This is a continual process since propositions are being created and modified all 
the time in the totality. So the propagators' mechanisms update the statistics of the proposition. 


00:57:42 1
The most fundamental associations of monads are those of orientation and temporality. Here 
orientation refers to whether the association is horizontal or vertical. Temporality refers 
to whether monads are activated in unison or in series. Holland, Holyoke, Thagard, and Nisbet, 
in 1989, explored synchronic, which means in unison, and diachronic, which means in series, 
associations. They said, mental models are best understood as assemblages of synchronic 
and diachronic rules organized into default hierarchies and clustered into categories. 
Creating simple associations, therefore, or involves making horizontal or vertical links 
among monads activated in unison or in series. Associators create horizontal and vertical 
associations. 

00:58:34 1
Chunkers perform clustering within the ontology. An ontology is an extended vertical association 
where multiple complex ideas are arranged into a hierarchy or heterarchy. Incremental heterarchical 
concept formation is a process that creates abstractions from a stream or set of experiences. 
These abstractions are concepts which form an ontology. If the process is incremental, then 
the ontology can grow as new experiences are processed. 

00:59:05 1
Pickett and Oates in 2005 developed the cruncher algorithm to generate heterarchical ontologies 
from a group of feature sets using the minimum description length principle.  

00:59:21 1
Both pre-conceptual and conceptual inference is supported. Entities, situations, ideas, 
events, and episodes that are not linked to conceptual categories serve as pre-concepts. 
The reasoners perform transductive inference over these pre-concepts. 

00:59:39 1
Transductive inference is from cases to cases, or from instances to instances. Once preconcepts 
become linked to conceptual categories, they are eligible to participate in conceptual inference, 
using Piagetian, which is infralogical or logical inference operations, or operations from 
Mikalski's inferential learning theory. 

01:00:04 1
Provers answer questions generated within the system. They determine whether one schema 
can be derived from a probe schema. 

01:00:13 1
Refuters answer questions generated within the system by searching for cases which disprove 
a probe schema.  

01:00:22 1
Solvers formulate plans or behaviors to fill in gaps between existing or hypothetical states 
and desired states.  

01:00:32 1
Reactors attempt reliable actions given the current state of affairs.  

01:00:41 1
Deliberators attempt useful actions given the current state of affairs.  

01:00:47 1
Arrangers continually lay out the schemes in the totality as schemes are created or forgotten. 
 

01:00:55 1
Mechanisms for reflection alter the behavior of the totality. Reflection is the process of 
modifying the behavior of the totality. 

01:01:13 1
The Attention mechanism reprioritizes the cognitive system's objectives.  

01:01:19 1
The Supervisor mechanism determines whether daydreaming is possible based on the number 
of unsatisfied, near-term objectives.  

01:01:27 1
The simulator mechanism selects actions in forward viewpoints.  

01:01:36 1
The inhibitor mechanism proves that action attempts will not be catastrophic, or it abandons 
these action attempts.  

01:01:47 1
The monitor mechanism tracks the system resources and utilization.  

01:02:03 1
The regulator corrects or reinforces behaviors.  

01:02:09 1
The correlator determines whether an attempted behavior achieved its intended result within 
the expected timeframe.  

01:02:30 1
The evaluator mechanism updates belief statistics about objectives, and produces an overall 
system sentiment.  

01:02:38 1
The explorer attempts novel behaviors with unknown results in order to learn. The predictor 
determines the forward model result if a behavior is attempted. The designer mechanism creates 
experiments. The experimenter mechanism performs experiments. The ascriber mechanism 
correlates effects with attempted behaviors. 

01:03:06 1
Consolidation mechanisms improve system performance via garbage collection.  

01:03:11 1
Consolidation conserves the cognitive system's memory.  

01:03:16 1
The compressor mechanism canonifies identical redundant structures, making them shared 
structures. The automator mechanism collapses repeating reliable behavior sequences into 
a single behavior. 

01:03:29 1
The amniator mechanism eliminates unreferenced ancient schemas.  

01:03:36 1
The process of reasoning adds inferences and refutes or proves propositions.  

01:03:44 1
Ryszard Michalski developed the inferential theory of learning, which states that learning 
is inferencing plus memorizing. Inferencing means any type of reasoning or knowledge transformation. 


01:03:59 1
In 1993, Michalski said, the fundamental equation for inference is P U BK derive C, or in English, 
premises and background knowledge entail conclusions. The premises P are a collection of 
propositions. The background knowledge BK is a collection of propositions, and the conclusions 
C are a collection of propositions. The process of reasoning involves applying a knowledge 
transmutation or transform operation to the premises, background knowledge, or conclusion 
in order to derive new propositions, which we call inferences. Inferences that are always 
true are called strong or conclusive inferences. Inferences that are plausible, partial, 
probabilistic, and that are not always true are called weak or contingent inferences. 

01:04:55 1
Analogy creates weak conclusions based on premises and background knowledge. Deduction 
creates both strong and weak conclusions based on premises and background knowledge. Deduction 
is truth-preserving, so true premises and true background knowledge will create strong conclusions. 
Induction creates both strong and weak conclusions based on conclusions and background knowledge. 


01:05:27 1
Once again, premises combined with background knowledge entails or produces conclusion. 
 

01:05:52 1
Let's take one example, Schema, from Michalsky's seminal paper. Dostoyevsky's paintings, 
Girl's Face and Lvov's Cathedral, are beautiful. 

01:06:04 1
The inference transforms presented in this table are a combination of rules from both Michalsky 
and Piaget. Transductive and inferological transformations are largely used for preconceptual 
reasoning, reasoning, while the remainder are used for conceptual reasoning. To perform 
an inductive generalization on this schema, the reasoner retrieves the inductor generalization 
transform, which in this case is a category C with some members having property y entails maybe 
all members of C have property y. The reasoner then constructs a new proposition via accommodation 
or proposition revision. Focusing on the category Dostky's paintings and property beautiful, 


01:06:51 1
the mechanism creates the new inductive generalization schema, distinct from the original 
but sharing some arguments.  

01:07:00 1
To infer, maybe all Dostky's paintings are beautiful.  

01:07:06 1
The premises of the reasoning process was the original statement. The Background knowledge 
was the inductive generalization rule, and the conclusion was the new schema. 

01:07:17 1
This inference is part of the totality for as long as it's required. It is new and active, but 
if not used, it will be forgotten and will need to be re-derived. 

01:07:41 1
Another example from Mikalski involves a larger network. Given the question, does one power 
plant in New York have mechanical failures? We use the retriever mechanism to retrieve the 
nearest matching schema. 

01:07:58 1
We note that this schema is an approximate match and not an exact match to the inquiry. The retriever 
hands off the schema to the prover. 

01:08:08 1
The retrieved schema says that some oil power plants in Syracuse have mechanical failures, 
and also provides the concomitant background knowledge.  

01:08:21 1
The prover mechanism constructs the inquiry schema, Does one power plant in New York have mechanical 
failures?  

01:08:31 1
The next task for the prover is to find a series of transforms which can make its statements comparable. 
 

01:08:38 1
The prover looks for a transform that can isolate the quantity descriptor 1.  

01:08:44 1
A deductive specialization transform can be employed to go from some members to one member. 
 

01:08:51 1
Using deductive specialization, we can transmit the quantity from some to one from the candidate 
background schema. The deductive specialization transform works. Next, it tries to make 
New York comparable. A deductive generalization transform can be used to go from Syracuse 
to New York in the inquiry. 

01:09:16 1
The prover now turns to the question of power plants.  

01:09:20 1
Using deductive generalization, the prover is able to generalize oil power plant to power 
plant.  

01:09:33 1
Finally, it turns to the question of mechanical failures. It finds an alternative equivalence 
transform, which states two propositions referring to the same category entails the reference 
are the same. 

01:09:49 1
Overall, the prover finds the statements are comparable because the inquiry can be transformed 
formed into the retrieved background knowledge. As a consequence, the inquiry itself can 
be stored in the totality as well and used for further reasoning. 

01:10:05 1
If it is not utilized, it will be forgotten. Since the statement can be proven, the prover responds 
in the affirmative. 

01:10:13 1
The probe schema is also added to the background knowledge of the totality.  

01:10:19 1
You can read more about the design patterns in the book Building Minds with Patterns available 
on Amazon.  

01:10:28 1
If you found this information useful, please like the video and subscribe to the channel, and 
visit us on Patreon. Become a member so that we can bring you more helpful content. Find us on 
the web at Linktree. If you need advice on systems you are building, book a consultation on Calendly. 
You can also email us at subthought at hotmail dot com. We appreciate your feedback in the comments 
or via email. We'll see you in the next video. 

01:11:25 1
you you You  

01:13:07 1
Oh, you're still here? Okay then, we can do a little more. 

01:13:16 1
If we were to build a cognitive system, we'd have to use something like this to code it. This is 
a prototype development environment called SubThought Studio. You enter your email address 
and password, and then we can begin to discuss how to code an ontology as described in this video. 
We can start by defining a reifier structure. 

01:13:38 1
In a development environment, we can load a file.  

01:13:42 1
We're using the subthought language, which is a derivative of Lisp. We've tossed around the 
names L-sharp or premise for brevity's sake. 

01:13:55 1
Source code files in this language are called theories and have the suffix th or.theory. Here 
we are groking the Ray of Fires theory. Groking means to interpret the file, or understand the 
file, or to load the file. The interpreter returns the last item in the file, typically the module 
name, and creates a tab to view the file. 

01:14:24 1
If you're familiar with the Lisp programming language, then you'll recognize all the parentheses. 
Function calls are colored green, symbols are magenta, slots and methods are prefixed with 
colons or exclamation points respectively, and are blue. Numbers are also blue. And literals 
are generally black. Strings are amber and comments are gray. This is a literal language as 
opposed to a symbolic language like Lisp, because unless Once prefixed with a question mark, 
every alphanumeric sequence evaluates to itself, hence the term literal. Symbols begin with 
question marks. 

01:14:59 1
The bottom pane shows the results of the evaluating each form.  

01:15:04 1
Why not code this in Python? Eventually, I will, but I just like this better, it's slick. 

01:15:13 1
The file in the tab is reifiers.theory, or reifiers.th to be more accurate.  

01:15:23 1
A modular symbol to keep track of reifiers is initialized to a 128-bit integer as 0. Long numbers 
are 128 bits. 

01:15:34 1
A relation is like a relational database table. A relation named usage is defined with two slots, 
reifier and count. The reifier slot has an unspecified default value of nil, while the count 
slot has a default value of 1. 

01:15:49 1
The reifier function is defined using the function function. Say that five times fast. The 
function function takes an optional name of a function. In this case, reifier. It takes an argument 
list. The plus sign indicates arguments following are optional. If a type is not specified, 
then the symbol is assumed to be a variant. Here, the default value of the sole parameter is 1. 


01:16:13 1
The next line is a comment about the purpose of the function. It's usually a good practice to 
put one in. If this is a string rather than a comment, then it could be used as a documentation 
string. 

01:16:26 1
Next, we define a critical section with a list of literals. Here we use the sole literal global 
monad update. The rest of the code in the critical section will be executed sequentially with 
locks on the literal. The taxonP function tests whether the next value is of the specified type 
in the type taxonomy. We're asking if the symbol args is a number. 

01:16:50 1
If arg happens to be a number, we will increment the reifier count by that number. Then we'll 
create a new usage record for that reifier and store the incremented value as the reifier. This 
allows us to potentially skip numbers. 

01:17:12 1
If the argument is not the literal count, then signal a failure. Otherwise, simply return the 
count of reifiers. 

01:17:22 1
When the function is defined, you'll see the results in the evaluation pane.  

01:17:28 1
Calling reifier the first time returns the number one. calling reifier the second time returns 
the number 2, requesting the count will return the number 2, reset will set the reifiers counter 
to 0. 

01:17:42 1
At the end of the module definition, the name of the module is returned. Partial modules can 
be created with the extend function rather than the module function. 

01:17:51 1
And that's about it for creating reifiers.  

01:17:55 1
Schemes are the next bit.  

01:17:58 1
Selecting the console tab gets us back to the interpreter. We can grok the schemes theory file. 


01:18:05 1
Then we can select the tab.  

01:18:09 1
The schemes module defines schemes for us. How nice. We can adjust the panel size so we can see 
more code. Now we can dig in. We will require the reifiers module as we'll need some definitions 
from it. We define a new relation, lexeme, to store our simple and compound words. We define 
the lexify function to store literals and return the corresponding reifier for that literal 
or literal expression, that is, a simple word or compound word. 

01:18:38 1
We adjust the editor a little bit to move to the next section.  

01:18:43 1
The afferent relation takes a reifier and an argument. It is defined to provide fast lookup 
between individual arguments and propositional reifiers. 

01:18:53 1
The efferent relationship maps reifiers to reference. The itemize function connects arguments 
and reference to a reifier using the new function, which either returns an existing record 
or creates a new one. 

01:19:06 1
Moving down a little bit, the scheme relation creates the foundational table for the neural 
propositions. The schemify function creates a neural proposition given a relationship literal, 
a list of arguments, and a list of reference. The new function creates a row in the underlying 
table. 

01:19:25 1
Then we're at the end of the file, that's about it.  

01:19:29 1
Back to the top, there you have the schemes theory file. Very simple, very concise, no mess, 
no fuss, nothing else to write, simple succinct code. Adjusting the panes, we can see the evaluated 
expressions. 

01:19:43 1
Returning to the console, we can inspect some of the structures we just created. If we view the 
scheme relation as a table, we can see what was created. 

01:19:56 1
If we view the scheme relation as a graph, we can see the slots associated with the relation in 
a network format.  

01:20:06 1
We can also clear the view so that we have a clean slate to continue with. Now we'll begin to code 
the totality, so you can see what that should look like. 

01:20:20 1
Back at the console, we'll grok the totality theory like so, then we'll turn to the tab.  

01:20:39 1
This theory requires a registry for configuration information.  

01:20:44 1
we'll store some values in the registry.  

01:20:49 1
The totality itself is a structure which contains the dimensions of the matrix as well as functions 
for layout of schemes.  

01:20:58 1
Moving down, we have relations to hold the scheme locations.  

01:21:03 1
In 1980, Minsky defined a knowledge line or K-line as an episodic link among agents in a memory. 
In a similar fashion, a viewpoint is an episodic link among monads across realities of the memory 
for a specific interval of time. The current viewpoint represents the observed reality during 
the lifespan of the system. 

01:21:31 1
An activation represents an instant of time. Anyway, this is a good beginning and it's enough 
for now. 

01:21:40 1
If you found this information useful, once again, please like the video and subscribe to the 
channel. And visit us on Patreon. Become a member so that we can bring you more helpful content. 
Find us on the web at Linktree. If you need advice on systems you're building, book a consultation 
on Calendly. And don't forget to send us an email and leave a comment. 

01:21:57 1
We'll see you in the next video.  